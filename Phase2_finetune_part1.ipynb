{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd6d61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install torch transformers datasets peft bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f7430d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2aed1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PHASE 2: FINE-TUNING WITH LORA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check GPU\n",
    "print(f\"\\nGPU available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# Model selection\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"  # Start with 1B (smaller, faster)\n",
    "# Later: \"meta-llama/Meta-Llama-3.1-8B\" or \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "print(f\"\\nLoading model: {model_name}\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token\n",
    "\n",
    "# Load model in 4-bit (QLoRA)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True  # QLoRA quantization\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ Model loaded successfully!\")\n",
    "print(f\"Model size: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B parameters\")\n",
    "\n",
    "# Test base model (before fine-tuning)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING BASE MODEL (Before Fine-tuning)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def generate_text(prompt, max_length=100):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=max_length,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"What are the symptoms of diabetes?\",\n",
    "    \"Explain how the heart works:\",\n",
    "    \"A patient has chest pain. What should I do?\"\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    response = generate_text(prompt, max_length=150)\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BASE MODEL TESTED\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- Base model gives general responses\")\n",
    "print(\"- Not specialized for medical domain\")\n",
    "print(\"- Next: Fine-tune with LoRA on medical data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f61b4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27397f19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae36315",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187837b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98616c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7618710f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
