{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2289b854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a75a159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04b52def",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab5c8e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading TinyStories dataset...\n"
     ]
    }
   ],
   "source": [
    "# Download TinyStories dataset (small sample for now)\n",
    "print(\"Downloading TinyStories dataset...\")\n",
    "\n",
    "# We'll use a smaller subset hosted on HuggingFace\n",
    "url = \"https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStories_all_data/data/train-00000-of-00004.parquet\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66235cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dataset created\n",
      "Total characters: 54,050\n",
      "Total words (approx): 10,950\n",
      "\n",
      "============================================================\n",
      "SAMPLE FROM DATASET:\n",
      "============================================================\n",
      "\n",
      "Once upon a time there was a little girl named Lily. Lily loved to play outside. One day she saw a big red ball. The ball was very bouncy.\n",
      "Lily picked up the ball and threw it high. The ball went up and up. Then it came down fast. Lily caught the ball and laughed.\n",
      "She played with the ball all day. When the sun went down, Lily went home. She was very happy. The end.\n",
      "\n",
      "Tom had a small cat. The cat was black and white. The cat liked to sleep. Every day the cat would find a sunny spot.\n",
      "The cat would\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "sample_stories = \"\"\"\n",
    "Once upon a time there was a little girl named Lily. Lily loved to play outside. One day she saw a big red ball. The ball was very bouncy.\n",
    "Lily picked up the ball and threw it high. The ball went up and up. Then it came down fast. Lily caught the ball and laughed.\n",
    "She played with the ball all day. When the sun went down, Lily went home. She was very happy. The end.\n",
    "\n",
    "Tom had a small cat. The cat was black and white. The cat liked to sleep. Every day the cat would find a sunny spot.\n",
    "The cat would curl up and close its eyes. Tom would pet the cat gently. The cat would purr loudly. Tom loved his cat very much.\n",
    "One day the cat chased a bird. The bird flew away. The cat came back to Tom. Tom gave the cat some food. The end.\n",
    "\n",
    "There was a boy named Sam. Sam liked to build things. He had many blocks. The blocks were different colors.\n",
    "Sam built a tall tower with the blocks. The tower was very high. Sam was proud of his tower. Then his sister came.\n",
    "She wanted to play too. Together they built an even bigger tower. It was the biggest tower ever. They were both happy. The end.\n",
    "\"\"\" * 50  # Repeat to get more data\n",
    "\n",
    "print(f\"✓ Dataset created\")\n",
    "print(f\"Total characters: {len(sample_stories):,}\")\n",
    "print(f\"Total words (approx): {len(sample_stories.split()):,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE FROM DATASET:\")\n",
    "print(\"=\"*60)\n",
    "print(sample_stories[:500])\n",
    "print(\"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06b58756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Saved to 'tiny_stories.txt'\n"
     ]
    }
   ],
   "source": [
    "# Save to file\n",
    "with open('tiny_stories.txt', 'w') as f:\n",
    "    f.write(sample_stories)\n",
    "\n",
    "print(\"\\n✓ Saved to 'tiny_stories.txt'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c079dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 54,050 characters\n",
      "\n",
      "============================================================\n",
      "BUILDING TOKENIZER\n",
      "============================================================\n",
      "\n",
      "Vocabulary size: 34\n",
      "Characters in vocab: ['\\n', ' ', ',', '.', 'E', 'H', 'I', 'L', 'O', 'S', 'T', 'W', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y']\n",
      "\n",
      "============================================================\n",
      "ENCODING EXAMPLE\n",
      "============================================================\n",
      "Original text: 'The cat sat'\n",
      "Encoded: [10, 19, 16, 1, 14, 12, 29, 1, 28, 12, 29]\n",
      "Decoded: 'The cat sat'\n",
      "\n",
      "✓ Encoding/decoding works correctly!\n",
      "\n",
      "============================================================\n",
      "CHARACTER → ID MAPPING (sample)\n",
      "============================================================\n",
      "'T' → 10\n",
      "'h' → 19\n",
      "'e' → 16\n",
      "' ' → 1\n",
      "'c' → 14\n",
      "\n",
      "============================================================\n",
      "ENCODING FULL DATASET\n",
      "============================================================\n",
      "Encoded dataset shape: torch.Size([54050])\n",
      "First 100 tokens: [0, 8, 24, 14, 16, 1, 30, 26, 25, 24, 1, 12, 1, 29, 20, 23, 16, 1, 29, 19, 16, 27, 16, 1, 32, 12, 28, 1, 12, 1, 22, 20, 29, 29, 22, 16, 1, 18, 20, 27, 22, 1, 24, 12, 23, 16, 15, 1, 7, 20, 22, 33, 3, 1, 7, 20, 22, 33, 1, 22, 25, 31, 16, 15, 1, 29, 25, 1, 26, 22, 12, 33, 1, 25, 30, 29, 28, 20, 15, 16, 3, 1, 8, 24, 16, 1, 15, 12, 33, 1, 28, 19, 16, 1, 28, 12, 32, 1, 12, 1]\n",
      "\n",
      "First 100 characters decoded:\n",
      "\n",
      "Once upon a time there was a little girl named Lily. Lily loved to play outside. One day she saw a \n",
      "\n",
      "✓ Tokenized data saved to 'tokenized_data.pt'\n",
      "\n",
      "============================================================\n",
      "STEP 2 COMPLETE ✓\n",
      "============================================================\n",
      "✓ Text converted to numbers\n",
      "✓ Can convert back to text\n",
      "✓ Ready for model training\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load our data\n",
    "with open('tiny_stories.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"Loaded {len(text):,} characters\")\n",
    "\n",
    "# Build character-level tokenizer\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BUILDING TOKENIZER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get all unique characters\n",
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(f\"\\nVocabulary size: {vocab_size}\")\n",
    "print(f\"Characters in vocab: {chars}\")\n",
    "\n",
    "# Create mappings\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ENCODING EXAMPLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test encoding\n",
    "sample = \"The cat sat\"\n",
    "encoded = [char_to_idx[ch] for ch in sample]\n",
    "decoded = ''.join([idx_to_char[i] for i in encoded])\n",
    "\n",
    "print(f\"Original text: '{sample}'\")\n",
    "print(f\"Encoded: {encoded}\")\n",
    "print(f\"Decoded: '{decoded}'\")\n",
    "print(f\"\\n✓ Encoding/decoding works correctly!\")\n",
    "\n",
    "# Show character to ID mapping for sample\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CHARACTER → ID MAPPING (sample)\")\n",
    "print(\"=\"*60)\n",
    "for ch in sample[:5]:\n",
    "    print(f\"'{ch}' → {char_to_idx[ch]}\")\n",
    "\n",
    "# Encode entire dataset\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ENCODING FULL DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "data = torch.tensor([char_to_idx[ch] for ch in text], dtype=torch.long)\n",
    "print(f\"Encoded dataset shape: {data.shape}\")\n",
    "print(f\"First 100 tokens: {data[:100].tolist()}\")\n",
    "\n",
    "# Show what those tokens mean\n",
    "print(f\"\\nFirst 100 characters decoded:\")\n",
    "print(''.join([idx_to_char[i.item()] for i in data[:100]]))\n",
    "\n",
    "# Save for next step\n",
    "torch.save({\n",
    "    'data': data,\n",
    "    'char_to_idx': char_to_idx,\n",
    "    'idx_to_char': idx_to_char,\n",
    "    'vocab_size': vocab_size\n",
    "}, 'tokenized_data.pt')\n",
    "\n",
    "print(\"\\n✓ Tokenized data saved to 'tokenized_data.pt'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 2 COMPLETE ✓\")\n",
    "print(\"=\"*60)\n",
    "print(\"✓ Text converted to numbers\")\n",
    "print(\"✓ Can convert back to text\")\n",
    "print(\"✓ Ready for model training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25bc6252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 54,050 characters\n",
      "\n",
      "============================================================\n",
      "BUILDING TOKENIZER\n",
      "============================================================\n",
      "\n",
      "Vocabulary size: 34\n",
      "Characters in vocab: ['\\n', ' ', ',', '.', 'E', 'H', 'I', 'L', 'O', 'S', 'T', 'W', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y']\n",
      "\n",
      "============================================================\n",
      "ENCODING EXAMPLE\n",
      "============================================================\n",
      "Original text: 'The cat sat'\n",
      "Encoded: [10, 19, 16, 1, 14, 12, 29, 1, 28, 12, 29]\n",
      "Decoded: 'The cat sat'\n",
      "\n",
      "✓ Encoding/decoding works correctly!\n",
      "\n",
      "============================================================\n",
      "CHARACTER → ID MAPPING (sample)\n",
      "============================================================\n",
      "'T' → 10\n",
      "'h' → 19\n",
      "'e' → 16\n",
      "' ' → 1\n",
      "'c' → 14\n",
      "\n",
      "============================================================\n",
      "ENCODING FULL DATASET\n",
      "============================================================\n",
      "Encoded dataset shape: torch.Size([54050])\n",
      "First 100 tokens: [0, 8, 24, 14, 16, 1, 30, 26, 25, 24, 1, 12, 1, 29, 20, 23, 16, 1, 29, 19, 16, 27, 16, 1, 32, 12, 28, 1, 12, 1, 22, 20, 29, 29, 22, 16, 1, 18, 20, 27, 22, 1, 24, 12, 23, 16, 15, 1, 7, 20, 22, 33, 3, 1, 7, 20, 22, 33, 1, 22, 25, 31, 16, 15, 1, 29, 25, 1, 26, 22, 12, 33, 1, 25, 30, 29, 28, 20, 15, 16, 3, 1, 8, 24, 16, 1, 15, 12, 33, 1, 28, 19, 16, 1, 28, 12, 32, 1, 12, 1]\n",
      "\n",
      "First 100 characters decoded:\n",
      "\n",
      "Once upon a time there was a little girl named Lily. Lily loved to play outside. One day she saw a \n",
      "\n",
      "✓ Tokenized data saved to 'tokenized_data.pt'\n",
      "\n",
      "============================================================\n",
      "STEP 2 COMPLETE ✓\n",
      "============================================================\n",
      "✓ Text converted to numbers\n",
      "✓ Can convert back to text\n",
      "✓ Ready for model training\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load our data\n",
    "with open('tiny_stories.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"Loaded {len(text):,} characters\")\n",
    "\n",
    "# Build character-level tokenizer\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BUILDING TOKENIZER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get all unique characters\n",
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(f\"\\nVocabulary size: {vocab_size}\")\n",
    "print(f\"Characters in vocab: {chars}\")\n",
    "\n",
    "# Create mappings\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ENCODING EXAMPLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test encoding\n",
    "sample = \"The cat sat\"\n",
    "encoded = [char_to_idx[ch] for ch in sample]\n",
    "decoded = ''.join([idx_to_char[i] for i in encoded])\n",
    "\n",
    "print(f\"Original text: '{sample}'\")\n",
    "print(f\"Encoded: {encoded}\")\n",
    "print(f\"Decoded: '{decoded}'\")\n",
    "print(f\"\\n✓ Encoding/decoding works correctly!\")\n",
    "\n",
    "# Show character to ID mapping for sample\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CHARACTER → ID MAPPING (sample)\")\n",
    "print(\"=\"*60)\n",
    "for ch in sample[:5]:\n",
    "    print(f\"'{ch}' → {char_to_idx[ch]}\")\n",
    "\n",
    "# Encode entire dataset\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ENCODING FULL DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "data = torch.tensor([char_to_idx[ch] for ch in text], dtype=torch.long)\n",
    "print(f\"Encoded dataset shape: {data.shape}\")\n",
    "print(f\"First 100 tokens: {data[:100].tolist()}\")\n",
    "\n",
    "# Show what those tokens mean\n",
    "print(f\"\\nFirst 100 characters decoded:\")\n",
    "print(''.join([idx_to_char[i.item()] for i in data[:100]]))\n",
    "\n",
    "# Save for next step\n",
    "torch.save({\n",
    "    'data': data,\n",
    "    'char_to_idx': char_to_idx,\n",
    "    'idx_to_char': idx_to_char,\n",
    "    'vocab_size': vocab_size\n",
    "}, 'tokenized_data.pt')\n",
    "\n",
    "print(\"\\n✓ Tokenized data saved to 'tokenized_data.pt'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 2 COMPLETE ✓\")\n",
    "print(\"=\"*60)\n",
    "print(\"✓ Text converted to numbers\")\n",
    "print(\"✓ Can convert back to text\")\n",
    "print(\"✓ Ready for model training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63271f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dcb5e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa70850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d38621",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
