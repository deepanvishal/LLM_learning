{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99943019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BUILDING GPT MODEL FROM SCRATCH\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL torch.utils.data.dataloader.DataLoader was not an allowed global by default. Please use `torch.serialization.add_safe_globals([DataLoader])` or the `torch.serialization.safe_globals([DataLoader])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnpicklingError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Load data info\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m checkpoint = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata_loaders.pt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m vocab_size = checkpoint[\u001b[33m'\u001b[39m\u001b[33mvocab_size\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     12\u001b[39m seq_len = checkpoint[\u001b[33m'\u001b[39m\u001b[33mseq_len\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Deepan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:1470\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1462\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[32m   1463\u001b[39m                     opened_zipfile,\n\u001b[32m   1464\u001b[39m                     map_location,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1467\u001b[39m                     **pickle_load_args,\n\u001b[32m   1468\u001b[39m                 )\n\u001b[32m   1469\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle.UnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1470\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle.UnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1471\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[32m   1472\u001b[39m             opened_zipfile,\n\u001b[32m   1473\u001b[39m             map_location,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1476\u001b[39m             **pickle_load_args,\n\u001b[32m   1477\u001b[39m         )\n\u001b[32m   1478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "\u001b[31mUnpicklingError\u001b[39m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL torch.utils.data.dataloader.DataLoader was not an allowed global by default. Please use `torch.serialization.add_safe_globals([DataLoader])` or the `torch.serialization.safe_globals([DataLoader])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BUILDING GPT MODEL FROM SCRATCH\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load data info\n",
    "checkpoint = torch.load('data_loaders.pt')\n",
    "vocab_size = checkpoint['vocab_size']\n",
    "seq_len = checkpoint['seq_len']\n",
    "\n",
    "print(f\"\\nVocabulary size: {vocab_size}\")\n",
    "print(f\"Sequence length: {seq_len}\")\n",
    "\n",
    "# Model hyperparameters\n",
    "embedding_dim = 128\n",
    "num_heads = 4\n",
    "num_layers = 4\n",
    "ff_dim = 512\n",
    "dropout = 0.1\n",
    "\n",
    "print(f\"\\nModel hyperparameters:\")\n",
    "print(f\"  Embedding dimension: {embedding_dim}\")\n",
    "print(f\"  Number of heads: {num_heads}\")\n",
    "print(f\"  Number of layers: {num_layers}\")\n",
    "print(f\"  Feed-forward dimension: {ff_dim}\")\n",
    "print(f\"  Dropout: {dropout}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPONENT 1: MULTI-HEAD ATTENTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embedding_dim % num_heads == 0\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embedding_dim // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "        self.W_k = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "        self.W_v = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "        self.W_o = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        # Split into multiple heads\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Attention\n",
    "        attention_scores = Q @ K.transpose(-2, -1) / (self.head_dim ** 0.5)\n",
    "        \n",
    "        if mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        attended = attention_weights @ V\n",
    "        \n",
    "        # Concatenate heads\n",
    "        attended = attended.transpose(1, 2).contiguous()\n",
    "        attended = attended.view(batch_size, seq_len, self.embedding_dim)\n",
    "        \n",
    "        output = self.W_o(attended)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "print(\"✓ Multi-Head Attention defined\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPONENT 2: FEED-FORWARD NETWORK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embedding_dim, ff_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(embedding_dim, ff_dim)\n",
    "        self.linear2 = nn.Linear(ff_dim, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Expand\n",
    "        x = self.linear1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Compress\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "print(\"✓ Feed-Forward Network defined\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPONENT 3: TRANSFORMER BLOCK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, ff_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention = MultiHeadAttention(embedding_dim, num_heads)\n",
    "        self.feed_forward = FeedForward(embedding_dim, ff_dim, dropout)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Attention with residual\n",
    "        attended, attn_weights = self.attention(self.norm1(x), mask)\n",
    "        x = x + self.dropout(attended)\n",
    "        \n",
    "        # Feed-forward with residual\n",
    "        ff_out = self.feed_forward(self.norm2(x))\n",
    "        x = x + self.dropout(ff_out)\n",
    "        \n",
    "        return x, attn_weights\n",
    "\n",
    "print(\"✓ Transformer Block defined\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPONENT 4: COMPLETE GPT MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_heads, num_layers, ff_dim, seq_len, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embeddings (BASELINE EMBEDDINGS CREATED HERE!)\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(seq_len, embedding_dim)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embedding_dim, num_heads, ff_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output\n",
    "        self.ln_f = nn.LayerNorm(embedding_dim)\n",
    "        self.head = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.shape\n",
    "        \n",
    "        # Create causal mask (prevent looking at future tokens)\n",
    "        mask = torch.tril(torch.ones(seq_len, seq_len)).view(1, 1, seq_len, seq_len)\n",
    "        mask = mask.to(x.device)\n",
    "        \n",
    "        # Embeddings\n",
    "        token_emb = self.token_embedding(x)\n",
    "        pos = torch.arange(seq_len, device=x.device)\n",
    "        pos_emb = self.position_embedding(pos)\n",
    "        \n",
    "        x = self.dropout(token_emb + pos_emb)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        attention_weights = []\n",
    "        for block in self.blocks:\n",
    "            x, attn = block(x, mask)\n",
    "            attention_weights.append(attn)\n",
    "        \n",
    "        # Output\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "        \n",
    "        return logits, attention_weights\n",
    "\n",
    "print(\"✓ Complete GPT model defined\")\n",
    "\n",
    "# Create model\n",
    "model = GPT(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    ff_dim=ff_dim,\n",
    "    seq_len=seq_len,\n",
    "    dropout=dropout\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL ARCHITECTURE\")\n",
    "print(\"=\"*60)\n",
    "print(model)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model size: ~{total_params * 4 / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Test forward pass\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST FORWARD PASS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "dummy_input = torch.randint(0, vocab_size, (2, seq_len))\n",
    "print(f\"Input shape: {dummy_input.shape}\")\n",
    "\n",
    "logits, attn_weights = model(dummy_input)\n",
    "print(f\"Output logits shape: {logits.shape}\")\n",
    "print(f\"Number of attention weight tensors: {len(attn_weights)}\")\n",
    "print(f\"Each attention weight shape: {attn_weights[0].shape}\")\n",
    "\n",
    "print(\"\\n✓ Forward pass successful!\")\n",
    "\n",
    "# Save model\n",
    "torch.save({\n",
    "    'model': model,\n",
    "    'embedding_dim': embedding_dim,\n",
    "    'num_heads': num_heads,\n",
    "    'num_layers': num_layers,\n",
    "    'ff_dim': ff_dim,\n",
    "    'vocab_size': vocab_size,\n",
    "    'seq_len': seq_len\n",
    "}, 'model_untrained.pt')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 4 COMPLETE ✓\")\n",
    "print(\"=\"*60)\n",
    "print(\"✓ Model architecture built\")\n",
    "print(\"✓ Baseline embeddings created (random)\")\n",
    "print(\"✓ Ready for training!\")\n",
    "print(\"\\nNext: We'll inspect these random embeddings before training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62d7706",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
